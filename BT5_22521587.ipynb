{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Value Iteration & Policy Iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in c:\\users\\zvc\\appdata\\roaming\\python\\python312\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\zvc\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\zvc\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\zvc\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\zvc\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ['FrozenLake-v1', 'FrozenLake8x8-v1', 'Taxi-v3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy, render=False):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy[state]\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        if render:\n",
    "            print(env.render())\n",
    "            time.sleep(0.5)\n",
    "            if not done:\n",
    "                display.clear_output(wait=True)\n",
    "        state = next_state\n",
    "\n",
    "    return (total_reward, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_times(env, policy, max_episodes):\n",
    "    success = 0\n",
    "    list_of_steps = []\n",
    "    for i in range(max_episodes):\n",
    "        total_reward, steps = play(env, policy)\n",
    "\n",
    "        if total_reward > 0:\n",
    "            success += 1\n",
    "            list_of_steps.append(steps)\n",
    "\n",
    "    print(f'Number of successes: {success}/{max_episodes}')\n",
    "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Policy Evaluation Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=0.9, theta=1e-13):\n",
    "    \n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v_temp = v[s]\n",
    "            action = policy[s]\n",
    "            v_new = 0\n",
    "            for prob, next_state, reward, _ in env.P[s][action]:\n",
    "                v_new += prob * (reward + gamma * v[next_state])\n",
    "            v[s] = v_new\n",
    "            delta = max(delta, abs(v[s] - v_temp))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Policy Iteration Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=0.9, theta=1e-13):\n",
    "    \n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        i += 1\n",
    "        v = policy_evaluation(env, policy, gamma, theta)\n",
    "        policy_stable = True\n",
    "        for s in range(env.observation_space.n):\n",
    "            old_action = policy[s]\n",
    "            action_values = np.zeros(env.action_space.n)\n",
    "            for a in range(env.action_space.n):\n",
    "                for prob, next_state, reward, _ in env.P[s][a]:\n",
    "                    action_values[a] += prob * (reward + gamma * v[next_state])\n",
    "            policy[s] = np.argmax(action_values)\n",
    "            if policy[s] != old_action:\n",
    "                policy_stable = False\n",
    "        if policy_stable:\n",
    "            print(f'Converged at {i}-th iteration.')\n",
    "            break\n",
    "\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_1 = []\n",
    "for i in range(len(envs)):\n",
    "    time_taken_1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at 6-th iteration.\n",
      "FrozenLake-v1: 0.07380294799804688s\n",
      "Converged at 10-th iteration.\n",
      "FrozenLake8x8-v1: 0.692147970199585s\n",
      "Converged at 18-th iteration.\n",
      "Taxi-v3: 17.65279221534729s\n"
     ]
    }
   ],
   "source": [
    "optimal_v_values = []\n",
    "optimal_policies = []\n",
    "for i, env_name in enumerate(envs):\n",
    "    env = gym.make(env_name, render_mode=\"ansi\")\n",
    "    start = time.time()\n",
    "    policy, v = policy_iteration(env)\n",
    "    optimal_policies.append(policy)\n",
    "    optimal_v_values.append(v)\n",
    "    end = time.time()\n",
    "    time_taken_1[i] += (end - start)\n",
    "    print(f\"{env_name}: {end - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake-v1: 0.07759213447570801s\n",
      "FrozenLake8x8-v1: 0.7214705467224121s\n",
      "Taxi-v3: 17.63923931121826s\n"
     ]
    }
   ],
   "source": [
    "#Trung bình thời gian chạy của 3 toy games sau 5 lần\n",
    "for i, env_name in enumerate(envs):\n",
    "    print(f\"{env_name}: {time_taken_1[i]/5}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06888615 0.06141054 0.07440682 0.05580409 0.09185022 0.\n",
      " 0.11220663 0.         0.14543286 0.2474946  0.29961593 0.\n",
      " 0.         0.3799342  0.63901926 0.        ]\n",
      "[0.00641104 0.00854808 0.01230044 0.01778942 0.02508214 0.03247089\n",
      " 0.03957134 0.04297844 0.00602405 0.00764512 0.01091162 0.01642654\n",
      " 0.02605411 0.03619409 0.0493547  0.05730461 0.00509024 0.0058532\n",
      " 0.00677534 0.         0.02557084 0.03882139 0.06763973 0.08435607\n",
      " 0.0042256  0.00476954 0.00581968 0.0078541  0.02036065 0.\n",
      " 0.09175501 0.12919111 0.00318093 0.00319659 0.00270488 0.\n",
      " 0.0344439  0.06195145 0.10901921 0.20969093 0.00186915 0.\n",
      " 0.         0.01085079 0.03250092 0.06304172 0.         0.36008773\n",
      " 0.00118046 0.         0.00137717 0.00366839 0.         0.11568671\n",
      " 0.         0.63051379 0.00088531 0.00077462 0.00092218 0.\n",
      " 0.13824885 0.32258065 0.61443932 0.        ]\n",
      "[ 89.47323891  32.81971401  55.26423891  37.57755845   8.43222921\n",
      "  32.81971401   8.43222921  15.28447953  32.81971401  18.09386122\n",
      "  55.26423891  21.2154998   12.75594298  18.09386122  12.75594298\n",
      "  37.57755845 100.52591945  37.57755845  62.51591945  42.86394891\n",
      "  79.52591945  28.53774704  48.73781945  32.81971401  10.48035311\n",
      "  37.57755845  10.48035311  18.09386122  28.53774704  15.28447953\n",
      "  48.73781945  18.09386122  15.28447953  21.2154998   15.28447953\n",
      "  42.86394891  89.47323891  42.86394891  55.26423891  48.73781945\n",
      "  42.86394891  12.75594298  24.68388374  15.28447953  24.68388374\n",
      "  70.57323891  24.68388374  37.57755845  24.68388374  12.75594298\n",
      "  42.86394891  15.28447953  18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  48.73781945  79.52591945  48.73781945  55.26423891\n",
      "  37.57755845  10.48035311  21.2154998   12.75594298  28.53774704\n",
      "  79.52591945  28.53774704  42.86394891  21.2154998   10.48035311\n",
      "  37.57755845  12.75594298  21.2154998   28.53774704  21.2154998\n",
      "  55.26423891  42.86394891  89.47323891  42.86394891  62.51591945\n",
      "  32.81971401   8.43222921  18.09386122  10.48035311  32.81971401\n",
      "  89.47323891  32.81971401  48.73781945  18.09386122   8.43222921\n",
      "  32.81971401  10.48035311  18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  37.57755845 100.52591945  37.57755845  55.26423891\n",
      "  79.52591945  28.53774704  48.73781945  32.81971401  10.48035311\n",
      "  37.57755845  10.48035311  18.09386122  37.57755845  21.2154998\n",
      "  62.51591945  24.68388374  15.28447953  21.2154998   15.28447953\n",
      "  42.86394891  89.47323891  42.86394891  70.57323891  48.73781945\n",
      "  70.57323891  24.68388374  42.86394891  28.53774704  12.75594298\n",
      "  42.86394891  12.75594298  21.2154998   32.81971401  18.09386122\n",
      "  55.26423891  21.2154998   18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  79.52591945  48.73781945  62.51591945  55.26423891\n",
      "  48.73781945  15.28447953  28.53774704  18.09386122  21.2154998\n",
      "  62.51591945  21.2154998   32.81971401  28.53774704  15.28447953\n",
      "  48.73781945  18.09386122  21.2154998   28.53774704  21.2154998\n",
      "  55.26423891  55.26423891  70.57323891  55.26423891  62.51591945\n",
      "  42.86394891  12.75594298  24.68388374  15.28447953  24.68388374\n",
      "  70.57323891  24.68388374  37.57755845  24.68388374  12.75594298\n",
      "  42.86394891  15.28447953  24.68388374  32.81971401  24.68388374\n",
      "  62.51591945  48.73781945  79.52591945  48.73781945  70.57323891\n",
      "  37.57755845  10.48035311  21.2154998   12.75594298  28.53774704\n",
      "  79.52591945  28.53774704  42.86394891  21.2154998   10.48035311\n",
      "  37.57755845  12.75594298  21.2154998   28.53774704  21.2154998\n",
      "  55.26423891  42.86394891  89.47323891  42.86394891  62.51591945\n",
      "  70.57323891  24.68388374  42.86394891  28.53774704  12.75594298\n",
      "  42.86394891  12.75594298  21.2154998   42.86394891  24.68388374\n",
      "  70.57323891  28.53774704  18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  79.52591945  48.73781945  79.52591945  55.26423891\n",
      "  62.51591945  21.2154998   37.57755845  24.68388374  15.28447953\n",
      "  48.73781945  15.28447953  24.68388374  37.57755845  21.2154998\n",
      "  62.51591945  24.68388374  21.2154998   28.53774704  21.2154998\n",
      "  55.26423891  70.57323891  55.26423891  70.57323891  62.51591945\n",
      "  55.26423891  18.09386122  32.81971401  21.2154998   18.09386122\n",
      "  55.26423891  18.09386122  28.53774704  32.81971401  18.09386122\n",
      "  55.26423891  21.2154998   24.68388374  32.81971401  24.68388374\n",
      "  62.51591945  62.51591945  62.51591945  62.51591945  70.57323891\n",
      "  48.73781945  15.28447953  28.53774704  18.09386122  21.2154998\n",
      "  62.51591945  21.2154998   32.81971401  28.53774704  15.28447953\n",
      "  48.73781945  18.09386122  28.53774704  37.57755845  28.53774704\n",
      "  70.57323891  55.26423891  70.57323891  55.26423891  79.52591945\n",
      "  42.86394891  12.75594298  24.68388374  15.28447953  24.68388374\n",
      "  70.57323891  24.68388374  37.57755845  24.68388374  12.75594298\n",
      "  42.86394891  15.28447953  24.68388374  32.81971401  24.68388374\n",
      "  62.51591945  48.73781945  79.52591945  48.73781945  70.57323891\n",
      "  62.51591945  21.2154998   37.57755845  24.68388374  10.48035311\n",
      "  37.57755845  10.48035311  18.09386122  48.73781945  28.53774704\n",
      "  79.52591945  32.81971401  15.28447953  21.2154998   15.28447953\n",
      "  42.86394891  70.57323891  42.86394891  89.47323891  48.73781945\n",
      "  55.26423891  18.09386122  32.81971401  21.2154998   12.75594298\n",
      "  42.86394891  12.75594298  21.2154998   32.81971401  18.09386122\n",
      "  55.26423891  21.2154998   18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  62.51591945  48.73781945  62.51591945  55.26423891\n",
      "  48.73781945  15.28447953  28.53774704  18.09386122  15.28447953\n",
      "  48.73781945  15.28447953  24.68388374  28.53774704  15.28447953\n",
      "  48.73781945  18.09386122  21.2154998   28.53774704  21.2154998\n",
      "  55.26423891  55.26423891  55.26423891  55.26423891  62.51591945\n",
      "  42.86394891  12.75594298  24.68388374  15.28447953  18.09386122\n",
      "  55.26423891  18.09386122  28.53774704  24.68388374  12.75594298\n",
      "  42.86394891  15.28447953  32.81971401  42.86394891  32.81971401\n",
      "  79.52591945  48.73781945  62.51591945  48.73781945  89.47323891\n",
      "  37.57755845  10.48035311  21.2154998   12.75594298  21.2154998\n",
      "  62.51591945  21.2154998   32.81971401  21.2154998   10.48035311\n",
      "  37.57755845  12.75594298  28.53774704  37.57755845  28.53774704\n",
      "  70.57323891  42.86394891  70.57323891  42.86394891  79.52591945\n",
      "  55.26423891  18.09386122  32.81971401  21.2154998    8.43222921\n",
      "  32.81971401   8.43222921  15.28447953  55.26423891  32.81971401\n",
      "  89.47323891  37.57755845  12.75594298  18.09386122  12.75594298\n",
      "  37.57755845  62.51591945  37.57755845 100.52591945  42.86394891\n",
      "  48.73781945  15.28447953  28.53774704  18.09386122  10.48035311\n",
      "  37.57755845  10.48035311  18.09386122  28.53774704  15.28447953\n",
      "  48.73781945  18.09386122  15.28447953  21.2154998   15.28447953\n",
      "  42.86394891  55.26423891  42.86394891  55.26423891  48.73781945\n",
      "  42.86394891  12.75594298  24.68388374  15.28447953  12.75594298\n",
      "  42.86394891  12.75594298  21.2154998   24.68388374  12.75594298\n",
      "  42.86394891  15.28447953  18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  48.73781945  48.73781945  48.73781945  55.26423891\n",
      "  37.57755845  10.48035311  21.2154998   12.75594298  15.28447953\n",
      "  48.73781945  15.28447953  24.68388374  21.2154998   10.48035311\n",
      "  37.57755845  12.75594298  37.57755845  48.73781945  37.57755845\n",
      "  89.47323891  42.86394891  55.26423891  42.86394891 100.52591945\n",
      "  32.81971401   8.43222921  18.09386122  10.48035311  18.09386122\n",
      "  55.26423891  18.09386122  28.53774704  18.09386122   8.43222921\n",
      "  32.81971401  10.48035311  32.81971401  42.86394891  32.81971401\n",
      "  79.52591945  37.57755845  62.51591945  37.57755845  89.47323891]\n"
     ]
    }
   ],
   "source": [
    "for i in optimal_v_values:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "[3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
      " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0]\n",
      "[4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
      " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
      " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
      " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
      " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3]\n"
     ]
    }
   ],
   "source": [
    "for i in optimal_policies:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, env_name in enumerate(envs):\n",
    "    env = gym.make(env_name, render_mode=\"ansi\")\n",
    "    play(env, optimal_policies[i], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successes: 778/1000\n",
      "Average number of steps: 41.80848329048843\n",
      "Number of successes: 740/1000\n",
      "Average number of steps: 73.72972972972973\n",
      "Number of successes: 1000/1000\n",
      "Average number of steps: 13.011\n"
     ]
    }
   ],
   "source": [
    "for i, env_name in enumerate(envs):\n",
    "    env = gym.make(env_name, render_mode=\"ansi\")\n",
    "    play_multiple_times(env, optimal_policies[i], 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Policy Evaluation Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
    "    # Initialize the values of all states to be 0\n",
    "    v_values = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        prev_v_values = np.copy(v_values)\n",
    "\n",
    "        # Update the value of each state\n",
    "        for state in range(env.observation_space.n):\n",
    "            action = policy[state]\n",
    "\n",
    "            # Compute the q-value of the action\n",
    "            q_value = 0\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
    "\n",
    "            v_values[state] = q_value # update v-value\n",
    "\n",
    "        # Check convergence\n",
    "        if np.all(np.isclose(v_values, prev_v_values)):\n",
    "            print(f'Converged at {i}-th iteration.')\n",
    "            break\n",
    "\n",
    "    return v_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Value Iteration Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iters=500, gamma=0.9):\n",
    "    # initialize\n",
    "    v_values = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        prev_v_values = np.copy(v_values)\n",
    "\n",
    "        # update the v-value for each state\n",
    "        for state in range(env.observation_space.n):\n",
    "            q_values = []\n",
    "\n",
    "            # compute the q-value for each action that we can perform at the state\n",
    "            for action in range(env.action_space.n):\n",
    "                q_value = 0\n",
    "                # loop through each possible outcome\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
    "\n",
    "                q_values.append(q_value)\n",
    "\n",
    "            # select the max q-values\n",
    "            best_action = np.argmax(q_values)\n",
    "            v_values[state] = q_values[best_action]\n",
    "\n",
    "        # check convergence\n",
    "        if np.all(np.isclose(v_values, prev_v_values)):\n",
    "            print(f'Converged at {i}-th iteration.')\n",
    "            break\n",
    "\n",
    "    return v_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_2 = []\n",
    "for i in range(len(envs)):\n",
    "    time_taken_2.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at 79-th iteration.\n",
      "FrozenLake-v1: 0.04886913299560547s\n",
      "Converged at 117-th iteration.\n",
      "FrozenLake8x8-v1: 0.25531625747680664s\n",
      "Converged at 116-th iteration.\n",
      "Taxi-v3: 2.639939785003662s\n"
     ]
    }
   ],
   "source": [
    "optimal_v_values = []\n",
    "for i, env_name in enumerate(envs):\n",
    "    env = gym.make(env_name, render_mode=\"ansi\")\n",
    "    start = time.time()\n",
    "    v = value_iteration(env)\n",
    "    optimal_v_values.append(v)\n",
    "    end = time.time()\n",
    "    time_taken_2[i] += (end - start)\n",
    "    print(f\"{env_name}: {end - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake-v1: 0.0486696720123291s\n",
      "FrozenLake8x8-v1: 0.30558276176452637s\n",
      "Taxi-v3: 2.655098867416382s\n"
     ]
    }
   ],
   "source": [
    "#Trung bình thời gian chạy của 3 toy games sau 5 lần\n",
    "for i, env_name in enumerate(envs):\n",
    "    print(f\"{env_name}: {time_taken_2[i]/5}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06888615 0.06141054 0.07440682 0.05580409 0.09185022 0.\n",
      " 0.11220663 0.         0.14543286 0.2474946  0.29961593 0.\n",
      " 0.         0.3799342  0.63901926 0.        ]\n",
      "[0.00641104 0.00854808 0.01230044 0.01778942 0.02508214 0.03247089\n",
      " 0.03957134 0.04297844 0.00602405 0.00764512 0.01091162 0.01642654\n",
      " 0.02605411 0.03619409 0.0493547  0.05730461 0.00509024 0.0058532\n",
      " 0.00677534 0.         0.02557084 0.03882139 0.06763973 0.08435607\n",
      " 0.0042256  0.00476954 0.00581968 0.0078541  0.02036065 0.\n",
      " 0.09175501 0.12919111 0.00318093 0.00319659 0.00270488 0.\n",
      " 0.0344439  0.06195145 0.10901921 0.20969093 0.00186915 0.\n",
      " 0.         0.01085079 0.03250092 0.06304172 0.         0.36008773\n",
      " 0.00118046 0.         0.00137717 0.00366839 0.         0.11568671\n",
      " 0.         0.63051379 0.00088531 0.00077462 0.00092218 0.\n",
      " 0.13824885 0.32258065 0.61443932 0.        ]\n",
      "[ 89.47323891  32.81971401  55.26423891  37.57755845   8.43222921\n",
      "  32.81971401   8.43222921  15.28447953  32.81971401  18.09386122\n",
      "  55.26423891  21.2154998   12.75594298  18.09386122  12.75594298\n",
      "  37.57755845 100.52591945  37.57755845  62.51591945  42.86394891\n",
      "  79.52591945  28.53774704  48.73781945  32.81971401  10.48035311\n",
      "  37.57755845  10.48035311  18.09386122  28.53774704  15.28447953\n",
      "  48.73781945  18.09386122  15.28447953  21.2154998   15.28447953\n",
      "  42.86394891  89.47323891  42.86394891  55.26423891  48.73781945\n",
      "  42.86394891  12.75594298  24.68388374  15.28447953  24.68388374\n",
      "  70.57323891  24.68388374  37.57755845  24.68388374  12.75594298\n",
      "  42.86394891  15.28447953  18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  48.73781945  79.52591945  48.73781945  55.26423891\n",
      "  37.57755845  10.48035311  21.2154998   12.75594298  28.53774704\n",
      "  79.52591945  28.53774704  42.86394891  21.2154998   10.48035311\n",
      "  37.57755845  12.75594298  21.2154998   28.53774704  21.2154998\n",
      "  55.26423891  42.86394891  89.47323891  42.86394891  62.51591945\n",
      "  32.81971401   8.43222921  18.09386122  10.48035311  32.81971401\n",
      "  89.47323891  32.81971401  48.73781945  18.09386122   8.43222921\n",
      "  32.81971401  10.48035311  18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  37.57755845 100.52591945  37.57755845  55.26423891\n",
      "  79.52591945  28.53774704  48.73781945  32.81971401  10.48035311\n",
      "  37.57755845  10.48035311  18.09386122  37.57755845  21.2154998\n",
      "  62.51591945  24.68388374  15.28447953  21.2154998   15.28447953\n",
      "  42.86394891  89.47323891  42.86394891  70.57323891  48.73781945\n",
      "  70.57323891  24.68388374  42.86394891  28.53774704  12.75594298\n",
      "  42.86394891  12.75594298  21.2154998   32.81971401  18.09386122\n",
      "  55.26423891  21.2154998   18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  79.52591945  48.73781945  62.51591945  55.26423891\n",
      "  48.73781945  15.28447953  28.53774704  18.09386122  21.2154998\n",
      "  62.51591945  21.2154998   32.81971401  28.53774704  15.28447953\n",
      "  48.73781945  18.09386122  21.2154998   28.53774704  21.2154998\n",
      "  55.26423891  55.26423891  70.57323891  55.26423891  62.51591945\n",
      "  42.86394891  12.75594298  24.68388374  15.28447953  24.68388374\n",
      "  70.57323891  24.68388374  37.57755845  24.68388374  12.75594298\n",
      "  42.86394891  15.28447953  24.68388374  32.81971401  24.68388374\n",
      "  62.51591945  48.73781945  79.52591945  48.73781945  70.57323891\n",
      "  37.57755845  10.48035311  21.2154998   12.75594298  28.53774704\n",
      "  79.52591945  28.53774704  42.86394891  21.2154998   10.48035311\n",
      "  37.57755845  12.75594298  21.2154998   28.53774704  21.2154998\n",
      "  55.26423891  42.86394891  89.47323891  42.86394891  62.51591945\n",
      "  70.57323891  24.68388374  42.86394891  28.53774704  12.75594298\n",
      "  42.86394891  12.75594298  21.2154998   42.86394891  24.68388374\n",
      "  70.57323891  28.53774704  18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  79.52591945  48.73781945  79.52591945  55.26423891\n",
      "  62.51591945  21.2154998   37.57755845  24.68388374  15.28447953\n",
      "  48.73781945  15.28447953  24.68388374  37.57755845  21.2154998\n",
      "  62.51591945  24.68388374  21.2154998   28.53774704  21.2154998\n",
      "  55.26423891  70.57323891  55.26423891  70.57323891  62.51591945\n",
      "  55.26423891  18.09386122  32.81971401  21.2154998   18.09386122\n",
      "  55.26423891  18.09386122  28.53774704  32.81971401  18.09386122\n",
      "  55.26423891  21.2154998   24.68388374  32.81971401  24.68388374\n",
      "  62.51591945  62.51591945  62.51591945  62.51591945  70.57323891\n",
      "  48.73781945  15.28447953  28.53774704  18.09386122  21.2154998\n",
      "  62.51591945  21.2154998   32.81971401  28.53774704  15.28447953\n",
      "  48.73781945  18.09386122  28.53774704  37.57755845  28.53774704\n",
      "  70.57323891  55.26423891  70.57323891  55.26423891  79.52591945\n",
      "  42.86394891  12.75594298  24.68388374  15.28447953  24.68388374\n",
      "  70.57323891  24.68388374  37.57755845  24.68388374  12.75594298\n",
      "  42.86394891  15.28447953  24.68388374  32.81971401  24.68388374\n",
      "  62.51591945  48.73781945  79.52591945  48.73781945  70.57323891\n",
      "  62.51591945  21.2154998   37.57755845  24.68388374  10.48035311\n",
      "  37.57755845  10.48035311  18.09386122  48.73781945  28.53774704\n",
      "  79.52591945  32.81971401  15.28447953  21.2154998   15.28447953\n",
      "  42.86394891  70.57323891  42.86394891  89.47323891  48.73781945\n",
      "  55.26423891  18.09386122  32.81971401  21.2154998   12.75594298\n",
      "  42.86394891  12.75594298  21.2154998   32.81971401  18.09386122\n",
      "  55.26423891  21.2154998   18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  62.51591945  48.73781945  62.51591945  55.26423891\n",
      "  48.73781945  15.28447953  28.53774704  18.09386122  15.28447953\n",
      "  48.73781945  15.28447953  24.68388374  28.53774704  15.28447953\n",
      "  48.73781945  18.09386122  21.2154998   28.53774704  21.2154998\n",
      "  55.26423891  55.26423891  55.26423891  55.26423891  62.51591945\n",
      "  42.86394891  12.75594298  24.68388374  15.28447953  18.09386122\n",
      "  55.26423891  18.09386122  28.53774704  24.68388374  12.75594298\n",
      "  42.86394891  15.28447953  32.81971401  42.86394891  32.81971401\n",
      "  79.52591945  48.73781945  62.51591945  48.73781945  89.47323891\n",
      "  37.57755845  10.48035311  21.2154998   12.75594298  21.2154998\n",
      "  62.51591945  21.2154998   32.81971401  21.2154998   10.48035311\n",
      "  37.57755845  12.75594298  28.53774704  37.57755845  28.53774704\n",
      "  70.57323891  42.86394891  70.57323891  42.86394891  79.52591945\n",
      "  55.26423891  18.09386122  32.81971401  21.2154998    8.43222921\n",
      "  32.81971401   8.43222921  15.28447953  55.26423891  32.81971401\n",
      "  89.47323891  37.57755845  12.75594298  18.09386122  12.75594298\n",
      "  37.57755845  62.51591945  37.57755845 100.52591945  42.86394891\n",
      "  48.73781945  15.28447953  28.53774704  18.09386122  10.48035311\n",
      "  37.57755845  10.48035311  18.09386122  28.53774704  15.28447953\n",
      "  48.73781945  18.09386122  15.28447953  21.2154998   15.28447953\n",
      "  42.86394891  55.26423891  42.86394891  55.26423891  48.73781945\n",
      "  42.86394891  12.75594298  24.68388374  15.28447953  12.75594298\n",
      "  42.86394891  12.75594298  21.2154998   24.68388374  12.75594298\n",
      "  42.86394891  15.28447953  18.09386122  24.68388374  18.09386122\n",
      "  48.73781945  48.73781945  48.73781945  48.73781945  55.26423891\n",
      "  37.57755845  10.48035311  21.2154998   12.75594298  15.28447953\n",
      "  48.73781945  15.28447953  24.68388374  21.2154998   10.48035311\n",
      "  37.57755845  12.75594298  37.57755845  48.73781945  37.57755845\n",
      "  89.47323891  42.86394891  55.26423891  42.86394891 100.52591945\n",
      "  32.81971401   8.43222921  18.09386122  10.48035311  18.09386122\n",
      "  55.26423891  18.09386122  28.53774704  18.09386122   8.43222921\n",
      "  32.81971401  10.48035311  32.81971401  42.86394891  32.81971401\n",
      "  79.52591945  37.57755845  62.51591945  37.57755845  89.47323891]\n"
     ]
    }
   ],
   "source": [
    "for i in optimal_v_values:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Policy Extraction Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_extraction(env, v_values, gamma=0.9):\n",
    "    # initialize\n",
    "    policy = np.zeros(env.observation_space.n, dtype=np.int32)\n",
    "\n",
    "    # loop through each state in the environment\n",
    "    for state in range(env.observation_space.n):\n",
    "        q_values = []\n",
    "        # loop through each action\n",
    "        for action in range(env.action_space.n):\n",
    "            q_value = 0\n",
    "            # loop each possible outcome\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                q_value += prob * (reward + gamma * v_values[next_state])\n",
    "\n",
    "            q_values.append(q_value)\n",
    "\n",
    "        # select the best action\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[state] = best_action\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZVC\\AppData\\Roaming\\Python\\Python312\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "optimal_policies = []\n",
    "for i, env_name in enumerate(envs):\n",
    "    env = gym.make(env_name, render_mode=\"ansi\")\n",
    "    policy = policy_extraction(env, optimal_v_values[i])\n",
    "    optimal_policies.append(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "[3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
      " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0]\n",
      "[4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
      " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
      " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
      " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
      " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3]\n"
     ]
    }
   ],
   "source": [
    "for i in optimal_policies:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, env_name in enumerate(envs):\n",
    "    env = gym.make(env_name, render_mode=\"ansi\")\n",
    "    play(env, optimal_policies[i], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successes: 773/1000\n",
      "Average number of steps: 43.31953428201811\n",
      "Number of successes: 729/1000\n",
      "Average number of steps: 76.34293552812072\n",
      "Number of successes: 1000/1000\n",
      "Average number of steps: 12.907\n"
     ]
    }
   ],
   "source": [
    "for i, env_name in enumerate(envs):\n",
    "    env = gym.make(env_name, render_mode=\"ansi\")\n",
    "    play_multiple_times(env, optimal_policies[i], 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Nhận xét:*\n",
    "- Cả Policy Iteration và Value Iteration đều là những thuật toán để tìm chính sách tối ưu. \n",
    "  + **Policy Iteration** sử dụng hai bước lặp lại: (1) *Policy Evaluation* để tính giá trị cho một chính sách cho trước, và (2) *Policy Iteration* để cập nhật chính sách dựa trên giá trị đó. Nó thường hội tụ nhanh hơn **Value Iteration**, nhưng mỗi vòng lặp tốn nhiều thời gian hơn vì phải tính toán giá trị hàm giá trị cho mỗi chính sách.\n",
    "  + **Value Iteration** tính toán trực tiếp giá trị hàm giá trị tối ưu cho mỗi trạng thái, và từ đó suy ra chính sách tối ưu. Mỗi vòng lặp của nó tốn ít thời gian hơn so với **Policy Iteration**, nhưng có thể cần nhiều vòng lặp hơn để hội tụ.\n",
    "- So sánh thời gian chạy của 2 thuật toán trong các toy games sau 5 lần: ở cả 3 toy games, thuật toán **Value Iteration** đều cho ra kết quả nhanh hơn so với thuật toán **Policy Iteration** cụ thể là **Value Iteration** (0.05; 0.3; 2.6) và **Policy Iteration** (0.07; 0.7; 17.7) tương ứng với các toy games (*FrozenLake-v1*; *FrozenLake8x8-v1*; *Taxi-v3*)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
